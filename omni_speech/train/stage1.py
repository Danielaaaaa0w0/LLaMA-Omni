from omni_speech.model.builder import load_pretrained_model,create_model
import argparse
import os
import torch
from torch.utils.data import Dataset, DataLoader
import whisper
from omni_speech.conversation import conv_templates
import ipdb  
import math
import json
from tqdm import tqdm
from omni_speech.datasets.preprocess import tokenizer_speech_token
from transformers import DataCollatorForLanguageModeling
from transformers import TrainingArguments
from transformers import Trainer
from tqdm import tqdm
import torch.optim as optim
import torch.optim as optim
from transformers import DataCollatorForSeq2Seq
from torch.nn.utils.rnn import pad_sequence




# ã€æœ€çµ‚ç‰ˆæœ¬çš„è‡ªè¨‚ Trainerã€‘
class PrintSamplerTrainer(Trainer):
    def _get_train_sampler(self) -> torch.utils.data.Sampler:
        # 1. è¦†å¯«é€™å€‹æ–¹æ³•ï¼Œæ””æˆªçœŸæ­£çš„ Sampler å»ºç«‹éç¨‹
        sampler = super()._get_train_sampler()
        
        # 2. åœ¨ Sampler è¢«å›å‚³ä½¿ç”¨å‰ï¼Œå°å‡ºå®ƒçš„é¡å‹
        print("\n" + "="*60)
        print("âœ… Inside .train() loop: Intercepted the ACTUAL Training Sampler!")
        print(f"   - The Sampler being used for the training loop is: {type(sampler)}")
        print("="*60 + "\n")
        
        # 3. æŠŠå»ºç«‹å¥½çš„ Sampler å›å‚³ï¼Œè®“è¨“ç·´æ­£å¸¸ç¹¼çºŒ
        return sampler

    def train(self, *args, **kwargs):
        # 4. è¦†å¯« train æ–¹æ³•ï¼ŒåŠ å…¥ã€Œè¨“ç·´å‰ã€çš„æª¢æŸ¥
        # é€™è£¡æœƒé¡¯ç¤ºå‡ºæˆ‘å€‘ä¹‹å‰çœ‹åˆ°çš„ã€æ¯”è¼ƒè¡¨å±¤çš„è³‡è¨Š
        initial_train_dataloader = self.get_train_dataloader()
        print("\n" + "="*55)
        print("ğŸ”¬ Pre-flight Check (Before the training loop starts)")
        print(f"   - Initially reported Training Sampler:   {type(initial_train_dataloader.sampler)}")
        print("="*55 + "\n")

        print("ğŸš€ Starting the training process... Watch for the interception message.\n")
        
        # 5. å‘¼å«åŸå§‹çš„ train æ–¹æ³•ã€‚ç•¶å®ƒåŸ·è¡Œæ™‚ï¼Œæœƒè§¸ç™¼ä¸Šé¢æˆ‘å€‘è¦†å¯«çš„ _get_train_sampler
        return super().train(*args, **kwargs)
    
    

# Custom dataset class

def collate_fn(batch):
    for i in range(len(batch)):
        batch[i]= batch[i].values()
        
    input_ids,labels,speech_tensors,speech_lengths = zip(*batch)
    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=128009)
    labels = pad_sequence(labels, batch_first=True, padding_value=128009)

    speech_tensors = torch.stack(speech_tensors, dim=0)
    speech_lengths = torch.stack(speech_lengths, dim=0)
    return {"input_ids":input_ids,"labels":labels, "speech":speech_tensors, "speech_lengths":speech_lengths}

class CustomDataset(Dataset):
    def __init__(self, questions, tokenizer, model_config, input_type, mel_size):
        self.questions = questions
        self.tokenizer = tokenizer
        self.model_config = model_config
        self.input_type = input_type
        self.mel_size = mel_size

    def __getitem__(self, index):
        item = self.questions[index]
        speech_file = item["speech"]
        qs = item["conversations"][0]["value"]
        re = item["conversations"][1]["value"]

        conv = conv_templates[args.conv_mode].copy()
        conv.append_message(conv.roles[0], qs)
        conv.append_message(conv.roles[1], re)
        prompt = conv.get_prompt()

        speech = whisper.load_audio(speech_file)
        if self.input_type == "raw":
            speech = torch.from_numpy(speech)
            if self.model_config.speech_normalize:
                speech = torch.nn.functional.layer_norm(speech, speech.shape)
        elif self.input_type == "mel":
            speech = whisper.pad_or_trim(speech)
            speech = whisper.log_mel_spectrogram(speech, n_mels=self.mel_size).permute(1, 0)
        input_ids = tokenizer_speech_token(prompt, self.tokenizer, return_tensors='pt')
        ret=dict(input_ids=input_ids,labels=input_ids, speech=speech.to(torch.bfloat16), speech_lengths=torch.LongTensor([speech.shape[0]]))
        return ret
    def __len__(self):
        return len(self.questions)
    
# DataLoader
def create_data_loader(questions, tokenizer, model_config, input_type, mel_size, batch_size=2, num_workers=1):
    # assert batch_size == 1, "batch_size must be 1"
    
    dataset = CustomDataset(questions, tokenizer, model_config, input_type, mel_size)
    #data_loader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False, collate_fn=collate_fn)
    return dataset


def split_list(lst, n):
    """Split a list into n (roughly) equal-sized chunks"""
    chunk_size = math.ceil(len(lst) / n)  # integer division
    return [lst[i:i+chunk_size] for i in range(0, len(lst), chunk_size)]


def get_chunk(lst, n, k):
    chunks = split_list(lst, n)
    return chunks[k]


def train_model(args):
    # è®¾ç½®æ¯å¼ å¡çš„device
    
    
    device = 'cuda' if torch.cuda.is_available() else 'cpu'     # è®¾ç½® deviceï¼Œèƒ½ç”¨ cuda å°±ç”¨ cudaï¼Œè‹¹æœ M ç³»åˆ—å¯ä»¥ç”¨ mps

    model_path = os.path.expanduser(args.model_path)
    tokenizer, model, context_len = create_model(model_path, args.model_base, is_lora=args.is_lora, s2s=args.s2s)

    
    questions = json.load(open(os.path.expanduser(args.question_file), "r"))
    questions = get_chunk(questions, args.num_chunks, args.chunk_idx) #chunk 1 chunk-idx 0 å–listä¸­çš„å¤šå°‘è¿›è¡Œæµ‹è¯•
    data_loader = create_data_loader(questions, tokenizer, model.config, args.input_type, args.mel_size)


    from transformers import Trainer, TrainingArguments
    # åˆå§‹åŒ–Trainer
    training_args = TrainingArguments(
    output_dir='saves3',                         # è¾“å‡ºè·¯å¾„ï¼ŒåŒ…æ‹¬æ¨¡å‹æ£€æŸ¥ç‚¹ã€ä¸­é—´æ–‡ä»¶ç­‰
        overwrite_output_dir=True,                  # æ˜¯å¦è¦†å†™ output_dir
        do_train=True,                              # æ˜¯å¦åšè®­ç»ƒ
        do_eval=False,                               # æ˜¯å¦åšè¯„ä¼°
        eval_steps=1,                            # è¯„ä¼°æ­¥éª¤é—´éš”
        per_device_train_batch_size=2,              # æ¯è®¾å¤‡æ‰¹æ¬¡
        gradient_accumulation_steps=6,              # æ¢¯åº¦ç´¯è®¡æ­¥å¤§å°ï¼Œçœæ˜¾å­˜ï¼Œä½†å°æ¨¡å‹æ²¡å¿…è¦ï¼Œç”¨ 1 æ”¶æ•›æ¯”è¾ƒå¿«
        learning_rate=1e-4,
        weight_decay=0.01,
        adam_beta2=0.95,
        warmup_ratio=0.01,
        lr_scheduler_type='cosine',                  # å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥ï¼ŒLLM è®­ç»ƒä¸€èˆ¬éƒ½ç”¨ä½™å¼¦
        logging_steps=100,                           # æ‰“å°æ­¥éª¤é—´éš”
        report_to="tensorboard",                              # æ—¥å¿—è¾“å‡ºç›®æ ‡ï¼Œä¸æƒ³ç”¨ wandb å¯ä»¥è®¾ç½®ä¸º None
        num_train_epochs=10,                         # è®­ç»ƒè½®æ•°ï¼Œ2 ~ 3 å³å¯
        save_steps=500,                            # æ£€æŸ¥ç‚¹ä¿å­˜æ­¥éª¤é—´éš”
        save_total_limit=150,                         # output_dir å†…ç•™å­˜çš„æ£€æŸ¥ç‚¹æœ€å¤§æ•°ç›®
        seed=3407,                                   # éšæœºç§å­
        bf16=True                                  # æ˜¯å¦å¼€å¯æ··åˆç²¾åº¦è®­ç»ƒ
        
    )
    tokenizer.pad_token = tokenizer.eos_token
    trainer = Trainer(
        model=model,
        tokenizer=tokenizer,
        args=training_args,
        train_dataset=data_loader,
        eval_dataset=data_loader,
        data_collator=collate_fn
    )

    trainer.train()
        


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser = argparse.ArgumentParser()
    parser.add_argument("--model-path", type=str, default="facebook/opt-350m")
    parser.add_argument("--model-base", type=str, default=None)
    parser.add_argument("--question-file", type=str)
    parser.add_argument("--answer-file", type=str)
    parser.add_argument("--conv-mode", type=str, default="v1")
    parser.add_argument("--num-chunks", type=int, default=1)
    parser.add_argument("--chunk-idx", type=int, default=0)
    parser.add_argument("--temperature", type=float, default=0)
    parser.add_argument("--top_p", type=float, default=None)
    parser.add_argument("--num_beams", type=int, default=1)
    parser.add_argument("--max_new_tokens", type=int, default=256)
    parser.add_argument("--input_type", type=str, default="raw")
    parser.add_argument("--mel_size", type=int, default=128)
    parser.add_argument("--s2s", action="store_true", default=False)
    parser.add_argument("--is_lora", action="store_true", default=False)
    args = parser.parse_args()
    train_model(args)